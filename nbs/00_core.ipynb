{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "import docker\n",
    "import typing\n",
    "import tarfile\n",
    "import jupytext\n",
    "import sched\n",
    "import time\n",
    "import importlib.util\n",
    "import inspect\n",
    "import ast\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Task(typing.TypedDict):\n",
    "    job_run_id: int\n",
    "    name: str\n",
    "    image: str | None\n",
    "    code_nb_path: str\n",
    "    applets_nb_path: str\n",
    "    input: dict\n",
    "\n",
    "def _get_source(task: Task, root: str):\n",
    "    spec = importlib.util.spec_from_file_location(\n",
    "        \"workflow\", f\"{root}/{task['code_nb_path']}\")\n",
    "    workflow_module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(workflow_module)\n",
    "    # getattr returns a DunderFunc\n",
    "    func_source = inspect.getsource(getattr(workflow_module, task['name']).func)\n",
    "    func_body = ast.parse(func_source).body[0].body\n",
    "    if not isinstance(func_body, list):\n",
    "        return ast.unparse(func_body)\n",
    "    func_body_drop_return = [el.value if isinstance(el, ast.Return) else el for el in func_body]\n",
    "    unparsed = map(ast.unparse, func_body_drop_return)\n",
    "    return \"\\n\".join(unparsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(_get_source({'code_nb_path': 'yo.py', 'name': 'task_one'}, \"./test_files\"), \"1\")\n",
    "test_eq(_get_source({'code_nb_path': 'yo.py', 'name': 'task_two'}, \"./test_files\"), \"1\")\n",
    "test_eq(_get_source({'code_nb_path': 'yo.py', 'name': 'task_three'}, \"./test_files\"), \"x = 1\\nx\")\n",
    "test_eq(_get_source({'code_nb_path': 'yo.py', 'name': 'task_four'}, \"./test_files\"), \"x = 1\\ny = 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_imports_source(path):\n",
    "    with open(path) as f:\n",
    "      source = f.read()\n",
    "      parsed_source = ast.parse(source)\n",
    "      return ast.unparse([n for n in ast.walk(parsed_source) if isinstance(n, (ast.Import, ast.ImportFrom))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(get_imports_source(\"test_files/job_8.py\"), \"from data_chimp_executor import execute as dchimp\\nimport pandas as pd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# TODO: The func list arg generation from input\n",
    "# is going to have to be more robust\n",
    "def _input_value_to_arg_value(value):\n",
    "    return f\"'value'\" if isinstance(value, str) else value\n",
    "\n",
    "def _task_to_script(task, automations, root):\n",
    "    task_source = _get_source(task, root)\n",
    "    imports_source = get_imports_source(f\"{root}/{task['code_nb_path']}\")\n",
    "    return f\"\"\"\\\n",
    "# %%\n",
    "{imports_source}\n",
    "import {task['wf_name']}\n",
    "\n",
    "{task['wf_name']}.{task['name']}({\", \".join([f\"{key}={_input_value_to_arg_value(value)}\" for key, value in task['input'].items()])})\n",
    "\n",
    "# %%\n",
    "import json\n",
    "dchimp.on_execute_cell('''{json.dumps({\"code\": task_source})}''', '{json.dumps(automations)}', globals() | json.loads('{json.dumps(task['input'])}'))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = _task_to_script({\n",
    "    'wf_name': 'my_workflow', \n",
    "    'name': 'task_one', \n",
    "    'code_nb_path': 'yo.py', \n",
    "    'input': {'arg_1': 1, 'arg_2': 'hello'}\n",
    "  },\n",
    "  [{\n",
    "    \"id\": \"plots\",\n",
    "    \"code\": \"df.plot.hist()\"\n",
    "  }],\n",
    "  'test_files'\n",
    ")\n",
    "  \n",
    "test_eq(actual, \"\"\"\\\n",
    "# %%\n",
    "from data_chimp_executor import execute as dchimp\n",
    "import my_workflow\n",
    "\n",
    "my_workflow.task_one(arg_1=1, arg_2='value')\n",
    "\n",
    "# %%\n",
    "import json\n",
    "dchimp.on_execute_cell('''{\"code\": \"1\"}''', '[{\"id\": \"plots\", \"code\": \"df.plot.hist()\"}]', globals() | json.loads('{\"arg_1\": 1, \"arg_2\": \"hello\"}'))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _update_task_status(host, task, status):\n",
    "    requests.post(\n",
    "        f\"{host}/updateTask/{task['job_run_id']}\",\n",
    "        json={\n",
    "            'task_name': task['name'],\n",
    "            'status': status\n",
    "        },\n",
    "        headers={'x-token': os.environ.get('CHIMP_TOKEN')}\n",
    "    ).raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _get_automations(path: str, root: str) -> list:\n",
    "    node = jupytext.read(f\"{root}/{path}\")\n",
    "    return [\n",
    "        {\"id\": \"\", \"code\": cell['source']}\n",
    "        for cell in node.cells\n",
    "        if cell['cell_type'] == \"code\" and \"dchimp.ignore\" not in cell['metadata'].get('tags', [])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(_get_automations(\"yo.ipynb\", \"./test_files\"), [{'id': '', 'code': '#| eval: false\\ndf.describe()'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "root = None\n",
    "def _execute():\n",
    "    if os.environ.get('CHIMP_TOKEN') is None:\n",
    "        print(\"CHIMP_TOKEN env variable missing\")\n",
    "        exit(1)\n",
    "    if os.environ.get('WORKFLOW_REPO') is None and os.environ.get('SOURCE_ROOT') is None:\n",
    "        print(\"WORKFLOW_REPO env variable missing\")\n",
    "        exit(1)\n",
    "    print(\"Polling for available jobs\")\n",
    "    host = os.environ.get(\"ORCHESTRATION_SERVER\", \"https://datachimp-api.onrender.com\")\n",
    "    \n",
    "    r = requests.get(f\"{host}/getTasks\",\n",
    "                     headers={'x-token': os.environ.get('CHIMP_TOKEN')})\n",
    "    r.raise_for_status()\n",
    "    tasks: typing.List[Task] = r.json()\n",
    "    global root\n",
    "    if len(tasks['data']) == 0:\n",
    "        print(\"no tasks. Trying again later...\")\n",
    "        root = None\n",
    "        return\n",
    "    if root is None:\n",
    "        root = f'workflow_repo_{int(time.time())}'\n",
    "    if os.environ.get('SOURCE_ROOT') is None:       \n",
    "        print(\"cloning repo\")\n",
    "        subprocess.run(['git', 'clone', os.environ.get(\"WORKFLOW_REPO\"), root])\n",
    "    task, *_ = tasks['data']\n",
    "    print(f\"received task: {task}\")\n",
    "    _update_task_status(host, task, 'started')\n",
    "    client = docker.from_env()\n",
    "    print(\"initiated docker client\")\n",
    "    image = task.get('image', 'jupyter/tensorflow-notebook')\n",
    "    print(\"pulling image\")\n",
    "    client.images.pull(image)\n",
    "    print(\"image pulled\")\n",
    "    # Transform code at code_nb_path into a notebook\n",
    "    script = _task_to_script(task,\n",
    "                             # TODO: Make it possible to run multiple applets that are selected by the user\n",
    "                             # just like you can toggle applets within VSCode\n",
    "                             _get_automations(\n",
    "                                 task['applets_nb_path'], os.environ.get('SOURCE_ROOT', root)\n",
    "                             ),\n",
    "                             os.environ.get('SOURCE_ROOT', root)\n",
    "                            )\n",
    "    jupytext.write(jupytext.reads(script, fmt=\"py:percent\"),\n",
    "                   \"data_chimp_notebook.ipynb\")\n",
    "    print(\"notebook created\")\n",
    "    container = client.containers.run(\n",
    "        image,\n",
    "        tty=True,\n",
    "        command=\"/bin/bash\",\n",
    "        detach=True\n",
    "    )\n",
    "    print(\"container started\")\n",
    "    # TODO: We'll need to copy more than just the module that has the workflow\n",
    "    # because the workflow could reference functions defined in other files.\n",
    "    # We'll probably have to pass the whole codebase. Would be nice if this was\n",
    "    # just a python package...maybe it could be if we assume usage of nbdev\n",
    "    with (\n",
    "        tarfile.open('code.tar', mode='w') as tar_f,\n",
    "    ):\n",
    "        tar_f.add(\n",
    "            f\"{os.environ.get('SOURCE_ROOT', root)}/{task['code_nb_path']}\",\n",
    "            f\"{task['wf_name']}.py\"\n",
    "        )\n",
    "        tar_f.add(\"data_chimp_notebook.ipynb\")\n",
    "    container.exec_run(\"mkdir -p data_chimp/source\")\n",
    "    with open('code.tar') as tar_f:\n",
    "        container.put_archive('/home/jovyan/data_chimp/source', tar_f)\n",
    "    container.exec_run(\n",
    "        \"cp data_chimp/source/data_chimp_notebook.ipynb data_chimp/source/data_chimp_notebook_writable.ipynb\"\n",
    "    )\n",
    "    container.exec_run(\n",
    "        \"wget https://github.com/getdatachimp/verus/raw/main/data_chimp_executor-0.1.0-py2.py3-none-any.whl\",\n",
    "        stream=True\n",
    "    )\n",
    "    container.exec_run('pip install data_chimp_executor-0.1.0-py2.py3-none-any.whl')\n",
    "    _update_task_status(host, task, 'env_ready')\n",
    "    print(\"source copied to container\")\n",
    "    # TODO: Run this function as a subprocess every few seconds so the orchestrator knows the build is still\n",
    "    # progressing\n",
    "    _update_task_status(host, task, 'executing')\n",
    "    container.exec_run(\n",
    "        'jupyter nbconvert --inplace --allow-errors --execute data_chimp/source/data_chimp_notebook_writable.ipynb'\n",
    "    )\n",
    "    print(\"finished task, sending results to data chimp\")\n",
    "    # Collect notebook results and post to orchestration server\n",
    "    _bytes, _ = container.get_archive(\n",
    "        '/home/jovyan/data_chimp/source/data_chimp_notebook_writable.ipynb'\n",
    "    )\n",
    "    with open('./sh_bin.tar', 'wb') as f:\n",
    "        for chunk in _bytes:\n",
    "            f.write(chunk)\n",
    "    with tarfile.open('./sh_bin.tar', mode='r') as f:\n",
    "        f.extractall(path=\"output\")\n",
    "    with open('output/data_chimp_notebook_writable.ipynb') as f:\n",
    "        nb = json.load(f)\n",
    "        r = requests.post(\n",
    "            f\"{host}/updateTask/{task['job_run_id']}\",\n",
    "            json={\n",
    "                'task_name': task['name'],\n",
    "                'status': 'done',\n",
    "                'nb': nb\n",
    "            },\n",
    "            headers={\n",
    "                'x-token': os.environ.get('CHIMP_TOKEN')\n",
    "            }\n",
    "        )\n",
    "        try: \n",
    "            r.raise_for_status()\n",
    "        except Exception as e:\n",
    "            print(\"failed to update task status because of error:\")\n",
    "            print(e)\n",
    "        finally:\n",
    "            container.stop()\n",
    "            if not os.environ.get('DC_DEBUG'):\n",
    "                container.remove()\n",
    "            print(\"task container stopped and removed\")\n",
    "\n",
    "\n",
    "def _run_every(func, sec=5):\n",
    "    s = sched.scheduler(time.time, time.sleep)\n",
    "\n",
    "    def do_something(sc):\n",
    "        func()\n",
    "        sc.enter(sec, 1, do_something, (sc,))\n",
    "\n",
    "    s.enter(sec, 1, do_something, (s,))\n",
    "    s.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def activate():\n",
    "    _run_every(_execute, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (main, Dec 12 2022, 12:11:53) [Clang 13.1.6 (clang-1316.0.21.2.5)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "13523e3cd1fbae48a3d0ca7b45a7084217927e918d7de376d1166dc0bc3d46c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
